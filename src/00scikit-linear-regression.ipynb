{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a series of notebooks focusing on applying the best practices of the `scikit-learn` API to standard ML tasks.\n",
    "The goal of these notebooks is to streamline the `scikit-learn` data science workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "df['MEDV'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description and Problem Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Boston Dataset Metadata description\n",
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the famous `boston` dataset. It contains features about residential properties that have been sold in the past. The goal is to use the features set to predict the target, `MEDV`, the median value of the residential property. This is a linear regression prediction task, where we try finding a line that best fits our training data and is a suitable predictor of the median value of new residential properties on the market, given their features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the training of our model, we have to perform a series of task that are important in ensuring that we are building an accurate and valid model. These data pre-processing steps are often a series of sanity checks that we perform to ensure that our data is in the right format and shape before being passed as an input to the Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing almost always involves the following steps - \n",
    "  1. Handling missing values.\n",
    "  2. Splitting the data into $X$ (**features**) and $y$ (**target**).\n",
    "  3. Feature scaling and standardization.\n",
    "  4. Label encoding.\n",
    "  5. Splitting the data into **training**, **testing** and **validation** sets.\n",
    "  6. Handling Imbalance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data does not have any missing values, we do not need to worry about imputing them in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our Data into $X$ and $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate out our dataset features into a matrix, $X$ and our target into a vector, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our features are numeric. Let's take a look at their numerical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features have values in varying orders of magnitude. While some are proportions, others are indices and some more are raw values. Since we are building a distance based linear regressor, we must apply standardization to our features.\n",
    "\n",
    "**Standardization** involves centering the variables so that the predictors have mean 0. This makes it easier to interpret the intercept term as the expected value of $Y_{i}$ when the predictor values are set to their means. Otherwise, the intercept is interpreted as the expected value of $Y_{i}$ when the predictors are set to 0, which may not be a realistic or interpretable.\n",
    "\n",
    "Another practical reason for scaling in regression is when one variable has a very large scale, In that case, the regression coefficients may be on a very small order of magnitude which can be a little annoying when you're reading computer output, so you may convert the variable to, for example, population size in millions. The convention that you standardize predictions primarily exists so that the units of the regression coefficients are the same. \n",
    "\n",
    "**Centering/scaling does not affect the statistical inference in regression models - the estimates are adjusted appropriately and the p-values will be the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This initializes an instance of the `StandardScalar` class\n",
    "# sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding/One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms cannot operate directly on labeled data. They require all input and output variables to be numeric. This is mostly a constraint of the efficient implementation of machine learning algorithms, since this allows implementations to store values using less disk space.\n",
    "\n",
    "All categorical data must be converted to numerical forms. If the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application.\n",
    "\n",
    "**LabelEncoder** can turn $[dog,cat,dog,mouse,cat]$ into $[1,2,1,3,2]$, but then the imposed ordinality means that the average of dog and mouse is cat.\n",
    "\n",
    "For categorical variables where no such ordinal relationship exists, **One-Hot-Encoding** must be used and has the advantage that the result is binary rather than ordinal.Everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and we start fighting with the curse of dimensionality. \n",
    "\n",
    "To fight the curse fo dimensionality, one could employ one-hot-encoding followed by PCA for dimensionality reduction. The combination of one-hot plus PCA can seldom be beat by other encoding schemes, since PCA finds the linear overlap and tends to group similar features into the same feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset doesn't have any categorical variables, we do not need to perform any label encoding on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13) (127, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379,) (127,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This initializes an instance of the LinearRegression Class\n",
    "# lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 29.782\n",
      "Coefficient of determination: 0.635\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error\n",
    "print('Mean squared error: %.3F' % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.3F' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, while developing a ML model, we split our dataset into 2 blocks.\n",
    "  - The first block of the data on which we develop our model is the **Training** set and comprises between $65$-$80\\%$ of the entire dataset.\n",
    "  - The second block is reserved for the purposes of generating metrics for the evaluation of the developed model. This reserved block is called the **Testing** dataset and comprises the remaining $15$ - $35\\%$ of the entire dataset.\n",
    "\n",
    "This method of developing ML models leads to more robust models as we try to find a balance between **bias** and **variance**. However, **k-Fold Cross Validation** goes a step further in the development of robust models.\n",
    "\n",
    "Visually, k-Fold cross validation operates as follows:\n",
    "\n",
    "![k-fold-cv](k-fold-cv.png)\n",
    "\n",
    "The basic concept involves choosing a different \"**fold**\" as the testing set, iterating through all $k$ possible folds and running the model with that configuration. This is resampling method used to evaluate machine learning models on a limited data sample, called $k$-fold cross validation. When a specific value for $k$ is chosen, it may be used in place of $k$ in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data, using a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[0.759, 0.73, 0.755, 0.586, 0.682, 0.774, 0.799, 0.626, 0.761, 0.798]"
      ],
      "text/plain": [
       "[0.759, 0.73, 0.755, 0.586, 0.682, 0.774, 0.799, 0.626, 0.761, 0.798]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "# create a KFold object with 10 splits \n",
    "folds = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=folds)\n",
    "# scores = cross_val_score(lm, X, y, scoring='r2', cv=folds)\n",
    "\n",
    "[round(i, 3) for i in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 110 out of 110 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=RFE(estimator=LinearRegression(copy_X=True,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      n_jobs=None,\n",
       "                                                      normalize=False),\n",
       "                           n_features_to_select=None, step=1, verbose=0),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'n_features_to_select': [3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                                   12, 13]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "\n",
    "# specify range of hyperparameters\n",
    "hyper_params = [{'n_features_to_select': list(range(3, 14))}]\n",
    "\n",
    "# specify model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm)             \n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_features_to_select</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_features_to_select': 3}</td>\n",
       "      <td>0.608034</td>\n",
       "      <td>0.554364</td>\n",
       "      <td>0.525570</td>\n",
       "      <td>-0.061178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.612106</td>\n",
       "      <td>0.577629</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.568277</td>\n",
       "      <td>0.563240</td>\n",
       "      <td>0.571648</td>\n",
       "      <td>0.558775</td>\n",
       "      <td>0.572160</td>\n",
       "      <td>0.014602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016210</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_features_to_select': 4}</td>\n",
       "      <td>0.783628</td>\n",
       "      <td>0.527313</td>\n",
       "      <td>0.682624</td>\n",
       "      <td>0.188205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654204</td>\n",
       "      <td>0.687087</td>\n",
       "      <td>0.669450</td>\n",
       "      <td>0.649090</td>\n",
       "      <td>0.649270</td>\n",
       "      <td>0.656742</td>\n",
       "      <td>0.654508</td>\n",
       "      <td>0.644141</td>\n",
       "      <td>0.658145</td>\n",
       "      <td>0.012821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014774</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_features_to_select': 5}</td>\n",
       "      <td>0.728287</td>\n",
       "      <td>0.550511</td>\n",
       "      <td>0.644909</td>\n",
       "      <td>0.246386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668764</td>\n",
       "      <td>0.695226</td>\n",
       "      <td>0.680233</td>\n",
       "      <td>0.659379</td>\n",
       "      <td>0.661212</td>\n",
       "      <td>0.666707</td>\n",
       "      <td>0.663784</td>\n",
       "      <td>0.660087</td>\n",
       "      <td>0.669918</td>\n",
       "      <td>0.011168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013911</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_features_to_select': 6}</td>\n",
       "      <td>0.657919</td>\n",
       "      <td>0.724499</td>\n",
       "      <td>0.725382</td>\n",
       "      <td>0.547767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749006</td>\n",
       "      <td>0.760842</td>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.746426</td>\n",
       "      <td>0.740801</td>\n",
       "      <td>0.752971</td>\n",
       "      <td>0.744295</td>\n",
       "      <td>0.746651</td>\n",
       "      <td>0.749970</td>\n",
       "      <td>0.005529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_features_to_select': 7}</td>\n",
       "      <td>0.667113</td>\n",
       "      <td>0.738316</td>\n",
       "      <td>0.738269</td>\n",
       "      <td>0.541492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752174</td>\n",
       "      <td>0.762707</td>\n",
       "      <td>0.759188</td>\n",
       "      <td>0.751269</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.759644</td>\n",
       "      <td>0.745958</td>\n",
       "      <td>0.748907</td>\n",
       "      <td>0.753358</td>\n",
       "      <td>0.005674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009776</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>8</td>\n",
       "      <td>{'n_features_to_select': 8}</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.744083</td>\n",
       "      <td>0.730488</td>\n",
       "      <td>0.536427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754050</td>\n",
       "      <td>0.766960</td>\n",
       "      <td>0.761023</td>\n",
       "      <td>0.751931</td>\n",
       "      <td>0.747217</td>\n",
       "      <td>0.761563</td>\n",
       "      <td>0.749446</td>\n",
       "      <td>0.752731</td>\n",
       "      <td>0.755493</td>\n",
       "      <td>0.005742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>9</td>\n",
       "      <td>{'n_features_to_select': 9}</td>\n",
       "      <td>0.707274</td>\n",
       "      <td>0.746289</td>\n",
       "      <td>0.725955</td>\n",
       "      <td>0.544468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758328</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.753592</td>\n",
       "      <td>0.753358</td>\n",
       "      <td>0.765362</td>\n",
       "      <td>0.752174</td>\n",
       "      <td>0.754291</td>\n",
       "      <td>0.758361</td>\n",
       "      <td>0.005577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006902</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_features_to_select': 10}</td>\n",
       "      <td>0.723295</td>\n",
       "      <td>0.727719</td>\n",
       "      <td>0.744922</td>\n",
       "      <td>0.560114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764452</td>\n",
       "      <td>0.771476</td>\n",
       "      <td>0.766468</td>\n",
       "      <td>0.760765</td>\n",
       "      <td>0.759685</td>\n",
       "      <td>0.772555</td>\n",
       "      <td>0.756366</td>\n",
       "      <td>0.756518</td>\n",
       "      <td>0.763042</td>\n",
       "      <td>0.005361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>11</td>\n",
       "      <td>{'n_features_to_select': 11}</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.723166</td>\n",
       "      <td>0.756178</td>\n",
       "      <td>0.581750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768837</td>\n",
       "      <td>0.775334</td>\n",
       "      <td>0.766512</td>\n",
       "      <td>0.760937</td>\n",
       "      <td>0.763910</td>\n",
       "      <td>0.781529</td>\n",
       "      <td>0.762260</td>\n",
       "      <td>0.761674</td>\n",
       "      <td>0.767411</td>\n",
       "      <td>0.006188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>12</td>\n",
       "      <td>{'n_features_to_select': 12}</td>\n",
       "      <td>0.757874</td>\n",
       "      <td>0.721604</td>\n",
       "      <td>0.756202</td>\n",
       "      <td>0.575573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768839</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.770549</td>\n",
       "      <td>0.765479</td>\n",
       "      <td>0.764048</td>\n",
       "      <td>0.781539</td>\n",
       "      <td>0.768835</td>\n",
       "      <td>0.765230</td>\n",
       "      <td>0.769642</td>\n",
       "      <td>0.005024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>13</td>\n",
       "      <td>{'n_features_to_select': 13}</td>\n",
       "      <td>0.758989</td>\n",
       "      <td>0.729568</td>\n",
       "      <td>0.755437</td>\n",
       "      <td>0.585537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768841</td>\n",
       "      <td>0.780118</td>\n",
       "      <td>0.776292</td>\n",
       "      <td>0.765479</td>\n",
       "      <td>0.764049</td>\n",
       "      <td>0.781814</td>\n",
       "      <td>0.769045</td>\n",
       "      <td>0.765245</td>\n",
       "      <td>0.771172</td>\n",
       "      <td>0.005979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.014545      0.001779         0.000842        0.000542   \n",
       "1        0.016210      0.001175         0.000909        0.000547   \n",
       "2        0.014774      0.001563         0.000976        0.000056   \n",
       "3        0.013911      0.000961         0.001100        0.000182   \n",
       "4        0.011742      0.000718         0.001215        0.000379   \n",
       "5        0.009776      0.001012         0.001074        0.000121   \n",
       "6        0.009109      0.000568         0.001114        0.000315   \n",
       "7        0.006902      0.001110         0.001209        0.000312   \n",
       "8        0.005514      0.000785         0.001099        0.000499   \n",
       "9        0.004196      0.000984         0.001197        0.000480   \n",
       "10       0.002400      0.000511         0.001106        0.000507   \n",
       "\n",
       "   param_n_features_to_select                        params  \\\n",
       "0                           3   {'n_features_to_select': 3}   \n",
       "1                           4   {'n_features_to_select': 4}   \n",
       "2                           5   {'n_features_to_select': 5}   \n",
       "3                           6   {'n_features_to_select': 6}   \n",
       "4                           7   {'n_features_to_select': 7}   \n",
       "5                           8   {'n_features_to_select': 8}   \n",
       "6                           9   {'n_features_to_select': 9}   \n",
       "7                          10  {'n_features_to_select': 10}   \n",
       "8                          11  {'n_features_to_select': 11}   \n",
       "9                          12  {'n_features_to_select': 12}   \n",
       "10                         13  {'n_features_to_select': 13}   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0            0.608034           0.554364           0.525570   \n",
       "1            0.783628           0.527313           0.682624   \n",
       "2            0.728287           0.550511           0.644909   \n",
       "3            0.657919           0.724499           0.725382   \n",
       "4            0.667113           0.738316           0.738269   \n",
       "5            0.691796           0.744083           0.730488   \n",
       "6            0.707274           0.746289           0.725955   \n",
       "7            0.723295           0.727719           0.744922   \n",
       "8            0.731250           0.723166           0.756178   \n",
       "9            0.757874           0.721604           0.756202   \n",
       "10           0.758989           0.729568           0.755437   \n",
       "\n",
       "    split3_test_score  ...  split2_train_score  split3_train_score  \\\n",
       "0           -0.061178  ...            0.572500            0.612106   \n",
       "1            0.188205  ...            0.654204            0.687087   \n",
       "2            0.246386  ...            0.668764            0.695226   \n",
       "3            0.547767  ...            0.749006            0.760842   \n",
       "4            0.541492  ...            0.752174            0.762707   \n",
       "5            0.536427  ...            0.754050            0.766960   \n",
       "6            0.544468  ...            0.758328            0.768212   \n",
       "7            0.560114  ...            0.764452            0.771476   \n",
       "8            0.581750  ...            0.768837            0.775334   \n",
       "9            0.575573  ...            0.768839            0.775641   \n",
       "10           0.585537  ...            0.768841            0.780118   \n",
       "\n",
       "    split4_train_score  split5_train_score  split6_train_score  \\\n",
       "0             0.577629            0.557500            0.568277   \n",
       "1             0.669450            0.649090            0.649270   \n",
       "2             0.680233            0.659379            0.661212   \n",
       "3             0.752778            0.746426            0.740801   \n",
       "4             0.759188            0.751269            0.744754   \n",
       "5             0.761023            0.751931            0.747217   \n",
       "6             0.765600            0.753592            0.753358   \n",
       "7             0.766468            0.760765            0.759685   \n",
       "8             0.766512            0.760937            0.763910   \n",
       "9             0.770549            0.765479            0.764048   \n",
       "10            0.776292            0.765479            0.764049   \n",
       "\n",
       "    split7_train_score  split8_train_score  split9_train_score  \\\n",
       "0             0.563240            0.571648            0.558775   \n",
       "1             0.656742            0.654508            0.644141   \n",
       "2             0.666707            0.663784            0.660087   \n",
       "3             0.752971            0.744295            0.746651   \n",
       "4             0.759644            0.745958            0.748907   \n",
       "5             0.761563            0.749446            0.752731   \n",
       "6             0.765362            0.752174            0.754291   \n",
       "7             0.772555            0.756366            0.756518   \n",
       "8             0.781529            0.762260            0.761674   \n",
       "9             0.781539            0.768835            0.765230   \n",
       "10            0.781814            0.769045            0.765245   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.572160         0.014602  \n",
       "1           0.658145         0.012821  \n",
       "2           0.669918         0.011168  \n",
       "3           0.749970         0.005529  \n",
       "4           0.753358         0.005674  \n",
       "5           0.755493         0.005742  \n",
       "6           0.758361         0.005577  \n",
       "7           0.763042         0.005361  \n",
       "8           0.767411         0.006188  \n",
       "9           0.769642         0.005024  \n",
       "10          0.771172         0.005979  \n",
       "\n",
       "[11 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
